"""
JamMa-FDAFT Demo Utilities (ä¿®æ­£ç‰ˆ)

JamMaã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹FDAFTçµ±åˆç‰ˆã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
demo/utils.pyã®FDAFTå¯¾å¿œç‰ˆï¼ˆå®Œå…¨äº’æ›æ€§ç¢ºä¿ï¼‰
"""

import os
import sys
import torch
from torch import nn

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.jamma.jamma import JamMa as JamMa_
from src.jamma.backbone import CovNextV2_nano
from src.jamma_fdaft.backbone_fdaft import FDAFTEncoder
from loguru import logger


class JamMaFDAFTDemo(nn.Module):
    """
    JamMa-FDAFT Demo Model
    
    JamMaã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦FDAFTç‰¹å¾´ã‚’å‡¦ç†
    demo/utils.pyã®JamMaã‚¯ãƒ©ã‚¹ã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆå®Œå…¨äº’æ›ï¼‰
    """
    
    def __init__(self, config, pretrained='official') -> None:
        super().__init__()
        
        print("ğŸ”§ JamMa-FDAFTãƒ‡ãƒ¢ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
        
        # è¨­å®šã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆconfig ãŒæ—¢ã«è¾æ›¸ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ï¼‰
        if hasattr(config, 'keys'):  # è¾æ›¸ã®å ´åˆ
            self.jamma_config = config
        else:  # YACSè¨­å®šã®å ´åˆ
            self.jamma_config = self._convert_config_to_dict(config)
        
        # FDAFTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ï¼ˆFDAFTè¨­å®šã‚’è¾æ›¸ã‹ã‚‰ä½œæˆï¼‰
        try:
            self.backbone = self._create_fdaft_backbone(self.jamma_config)
            print("âœ… FDAFTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ FDAFTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ConvNextV2ã‚’ä½¿ç”¨
            print("ğŸ”„ ConvNextV2ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            self.backbone = CovNextV2_nano()

        # JamMaãƒãƒƒãƒãƒ£ãƒ¼ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã®JamMaã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ï¼‰
        try:
            self.matcher = JamMa_(self.jamma_config)
            print("âœ… JamMaãƒãƒƒãƒãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ JamMaãƒãƒƒãƒãƒ£ãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            raise
        
        # JamMaã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self._load_pretrained_weights(pretrained)

    def _convert_config_to_dict(self, yacs_config):
        """YACSè¨­å®šã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        try:
            jamma_cfg = yacs_config.JAMMA
            
            config_dict = {
                'coarse': {
                    'd_model': jamma_cfg.COARSE.D_MODEL,
                },
                'fine': {
                    'd_model': jamma_cfg.FINE.D_MODEL,
                    'dsmax_temperature': getattr(jamma_cfg.FINE, 'DSMAX_TEMPERATURE', 0.1),
                    'thr': jamma_cfg.FINE.THR,
                    'inference': jamma_cfg.FINE.INFERENCE
                },
                'match_coarse': {
                    'thr': jamma_cfg.MATCH_COARSE.THR,
                    'use_sm': jamma_cfg.MATCH_COARSE.USE_SM,
                    'border_rm': jamma_cfg.MATCH_COARSE.BORDER_RM,
                    'dsmax_temperature': getattr(jamma_cfg.MATCH_COARSE, 'DSMAX_TEMPERATURE', 0.1),
                    'inference': jamma_cfg.MATCH_COARSE.INFERENCE,
                    'train_coarse_percent': getattr(jamma_cfg.MATCH_COARSE, 'TRAIN_COARSE_PERCENT', 0.3),
                    'train_pad_num_gt_min': getattr(jamma_cfg.MATCH_COARSE, 'TRAIN_PAD_NUM_GT_MIN', 20)
                },
                'fine_window_size': jamma_cfg.FINE_WINDOW_SIZE,
                'resolution': list(jamma_cfg.RESOLUTION)  # tupleã‚’listã«å¤‰æ›
            }
            
            return config_dict
            
        except Exception as e:
            print(f"è¨­å®šå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            return {
                'coarse': {
                    'd_model': 256,
                },
                'fine': {
                    'd_model': 64,
                    'dsmax_temperature': 0.1,
                    'thr': 0.1,
                    'inference': True
                },
                'match_coarse': {
                    'thr': 0.2,
                    'use_sm': True,
                    'border_rm': 2,
                    'dsmax_temperature': 0.1,
                    'inference': True,
                    'train_coarse_percent': 0.3,
                    'train_pad_num_gt_min': 20
                },
                'fine_window_size': 5,
                'resolution': [8, 2]
            }

    def _create_fdaft_backbone(self, config):
        """JamMaè¨­å®šã‹ã‚‰FDAFTè¨­å®šã‚’ä½œæˆã—ã¦FDAFTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’åˆæœŸåŒ–"""
        # FDAFTã«å¿…è¦ãªè¨­å®šã‚’ä½œæˆ
        fdaft_config = type('FDAFTConfig', (), {})()
        fdaft_config.FDAFT = type('FDAFT', (), {})()
        fdaft_config.JAMMA = type('JAMMA', (), {})()
        fdaft_config.JAMMA.COARSE = type('COARSE', (), {})()
        fdaft_config.JAMMA.FINE = type('FINE', (), {})()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        fdaft_config.FDAFT.NUM_LAYERS = 3
        fdaft_config.FDAFT.SIGMA_0 = 1.0
        fdaft_config.FDAFT.USE_STRUCTURED_FORESTS = True
        fdaft_config.FDAFT.MAX_KEYPOINTS = 2000
        fdaft_config.FDAFT.NMS_RADIUS = 5
        
        # JamMaè¨­å®šã‹ã‚‰æ¬¡å…ƒã‚’å–å¾—
        fdaft_config.JAMMA.COARSE.D_MODEL = config['coarse']['d_model']
        fdaft_config.JAMMA.FINE.D_MODEL = config['fine']['d_model']
        fdaft_config.JAMMA.RESOLUTION = config['resolution']
        fdaft_config.JAMMA.FINE_WINDOW_SIZE = config['fine_window_size']
        
        return FDAFTEncoder.from_config(fdaft_config)

    def _load_pretrained_weights(self, pretrained):
        """JamMaã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if pretrained == 'official':
            try:
                print("ğŸ“¥ JamMaå…¬å¼å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                # JamMaã®å…¬å¼å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                state_dict = torch.hub.load_state_dict_from_url(
                    'https://github.com/leoluxxx/JamMa/releases/download/v0.1/jamma.ckpt',
                    file_name='jamma.ckpt'
                )['state_dict']
                
                # ãƒãƒƒãƒãƒ£ãƒ¼éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                matcher_state_dict = {}
                for key, value in state_dict.items():
                    if key.startswith('matcher.'):
                        # matcher.xxx -> xxx ã«å¤‰æ›
                        new_key = key[8:]  # "matcher."ã‚’é™¤å»
                        matcher_state_dict[new_key] = value
                
                # ãƒãƒƒãƒãƒ£ãƒ¼ã®ã¿ã«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¯é™¤å¤–ï¼‰
                missing_keys, unexpected_keys = self.matcher.load_state_dict(
                    matcher_state_dict, strict=False
                )
                
                logger.info(f"âœ“ JamMa official weights loaded successfully")
                logger.info(f"  Missing keys (expected for FDAFT): {len(missing_keys)}")
                logger.info(f"  Unexpected keys: {len(unexpected_keys)}")
                print("âœ… JamMaå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
                
            except Exception as e:
                logger.warning(f"Failed to load official JamMa weights: {e}")
                logger.info("Using random initialization")
                print(f"âš ï¸ JamMaå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                print("ğŸ”„ ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰åˆæœŸåŒ–ã—ã¾ã™")
                
        elif pretrained:
            try:
                # ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                state_dict = torch.load(pretrained, map_location='cpu')['state_dict']
                
                # ãƒãƒƒãƒãƒ£ãƒ¼éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                matcher_state_dict = {}
                for key, value in state_dict.items():
                    if key.startswith('matcher.'):
                        new_key = key[8:]
                        matcher_state_dict[new_key] = value
                
                missing_keys, unexpected_keys = self.matcher.load_state_dict(
                    matcher_state_dict, strict=False
                )
                
                logger.info(f"âœ“ JamMa weights loaded from: {pretrained}")
                logger.info(f"  Missing keys (expected for FDAFT): {len(missing_keys)}")
                logger.info(f"  Unexpected keys: {len(unexpected_keys)}")
                print(f"âœ… JamMaé‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†: {pretrained}")
                
            except Exception as e:
                logger.warning(f"Failed to load JamMa weights from {pretrained}: {e}")
                logger.info("Using random initialization")
                print(f"âš ï¸ JamMaé‡ã¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")

    def forward(self, data):
        """Forward pass - demo/utils.pyã®JamMaã‚¯ãƒ©ã‚¹ã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        # 1. FDAFTç‰¹å¾´æŠ½å‡º
        self.backbone(data)
        
        # 2. JamMaãƒãƒƒãƒãƒ³ã‚°ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
        return self.matcher(data)


# JamMaäº’æ›ã®è¨­å®šï¼ˆdemo/utils.pyã¨åŒã˜å½¢å¼ï¼‰
cfg = {
    'coarse': {
        'd_model': 256,
    },
    'fine': {
        'd_model': 64,
        'dsmax_temperature': 0.1,
        'thr': 0.1,
        'inference': True
    },
    'match_coarse': {
        'thr': 0.2,
        'use_sm': True,
        'border_rm': 2,
        'dsmax_temperature': 0.1,
        'inference': True
    },
    'fine_window_size': 5,
    'resolution': [8, 2]
}


# ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆdemo/utils.pyã¨åŒã˜åå‰ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã‚‹ã‚ˆã†ã«ï¼‰
JamMa = JamMaFDAFTDemo